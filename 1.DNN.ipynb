{
 "metadata": {
  "name": "",
  "signature": "sha256:33c94b52ad404a65c4738e87338e0dd7773d2b93f25103ca89d17be2aa59257c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "matplotlib.pyplot.gray()\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "rng = np.random.RandomState(42)\n",
      "\n",
      "import sys\n",
      "import time\n",
      "\n",
      "theano.config.floatX = 'float32'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will make heavy use of the resources in the Theano deep learning tutorial. We have it integrated into our git repository as a submodule. You can clone the git repository by doing the following steps:\n",
      "(be sure to include the `--recursive` or you won't get the Theano deep learning tutorial)\n",
      "\n",
      "`git clone --recursive https://github.com/graphific/DL-Meetup-intro.git`\n",
      "\n",
      "If you already cloned the repository, but the `DeepLearningTutorial` folder is empty, you need to checkout the submodule. Make sure you are in the folder `ComputeFest2015_DeepLearning` and then execute the following command:\n",
      "\n",
      "`git submodule update --init --recursive`\n",
      "\n",
      "\n",
      "Now we have to add this directory to the PythonPath. Depending on the location of your git repository you might have to change this path."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#be sure to include Lisa lab code to make it easier for us:\n",
      "sys.path.insert(1,'./DeepLearningTutorials/code')\n",
      "#sys.path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load MNIST dataset\n",
      "from logistic_sgd import load_data\n",
      "dataset='mnist.pkl.gz'\n",
      "\n",
      "## If not already cached this function actually downloads the data\n",
      "datasets = load_data(dataset)\n",
      "\n",
      "train_set_x, train_set_y = datasets[0]\n",
      "valid_set_x, valid_set_y = datasets[1]\n",
      "test_set_x, test_set_y = datasets[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "theano's shared variable = genius\n",
      "\n",
      "data can be shared between the CPU and the GPU without you having to write a single line of code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print train_set_x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print train_set_x.get_value()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Size of the training data matrix: \", train_set_x.get_value().shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import tile_raster_images\n",
      "\n",
      "samples = tile_raster_images(train_set_x.get_value(), img_shape=(28,28), \n",
      "                             tile_shape=(3,10), tile_spacing=(0, 0),\n",
      "                             scale_rows_to_unit_interval=True,\n",
      "                             output_pixel_vals=True)\n",
      "\n",
      "plt.imshow(samples)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data Normalization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# estimate the mean and std dev from the training data\n",
      "# then use these estimates to normalize the data\n",
      "# estimate the mean and std dev from the training data\n",
      "\n",
      "norm_mean = train_set_x.mean(axis=0)\n",
      "train_set_x = train_set_x - norm_mean\n",
      "norm_std = train_set_x.std(axis=0)\n",
      "norm_std = norm_std.clip(0.00001, norm_std.max())\n",
      "train_set_x = train_set_x / norm_std \n",
      "\n",
      "test_set_x = test_set_x - norm_mean\n",
      "test_set_x = test_set_x / norm_std \n",
      "valid_set_x = valid_set_x - norm_mean\n",
      "valid_set_x = valid_set_x / norm_std "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize the result\n",
      "samples = tile_raster_images(train_set_x.eval(), img_shape=(28,28), \n",
      "                             tile_shape=(3,10), tile_spacing=(0, 0),\n",
      "                             scale_rows_to_unit_interval=True,\n",
      "                             output_pixel_vals=True)\n",
      "\n",
      "plt.imshow(samples)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_x = theano.shared(train_set_x.eval())\n",
      "valid_set_x = theano.shared(valid_set_x.eval())\n",
      "test_set_x = theano.shared(test_set_x.eval())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "logistic regression model has the weight parameters W and the biases b. \n",
      "We need to intialize these as Theano variables. \n",
      "\n",
      "In addition we create placeholders for our data by declaring a data matrix x, and a label vector y. Note that we do not assign our actual training data to the variables x and y. Theano can create the whole computational graph for x and y without knowing the actual values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 1. Declare Theano variables\n",
      "x = T.fmatrix()\n",
      "y = T.ivector()\n",
      "# size of our image patches, and number of output units (classes)\n",
      "W = theano.shared(value=np.zeros((28*28, 10),                            \n",
      "                dtype=theano.config.floatX),\n",
      "            name='W',\n",
      "            borrow=True\n",
      "        )\n",
      "b = theano.shared(value=np.zeros((10,), # number of output units (classes)\n",
      "                dtype=theano.config.floatX),\n",
      "            name='b',\n",
      "            borrow=True\n",
      "        )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 2. Construct Theano expression graph\n",
      "\n",
      "#likelihood to make predictions\n",
      "p_y_given_x = T.nnet.softmax(T.dot(x, W) + b)\n",
      "\n",
      "#conversion of the class probabilities to class labels\n",
      "y_pred = T.argmax(p_y_given_x, axis=1)\n",
      "\n",
      "#classification error\n",
      "error = T.mean(T.neq(y_pred, y))\n",
      "\n",
      "#the negative log likelihood as our cost function to optimize\n",
      "cost = -T.mean(T.log(p_y_given_x)[T.arange(y.shape[0]), y])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "compute gradients"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "we let thenao do the nasty work of computing the gradient with \"T.grad()\"\n",
      "on our cost function\n",
      "- specify with respect to which parameter the derivative should be computed. \n",
      "- use the computed gradient to perform an iteration of gradient decent. \n",
      "- update the parameters by taking a small step into the right direction on the cost function surface. \n",
      "- The size of the step is specified by the learning rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g_W = T.grad(cost=cost, wrt=W)\n",
      "g_b = T.grad(cost=cost, wrt=b)\n",
      "\n",
      "learning_rate = 0.1\n",
      "\n",
      "updates = [(W, W - learning_rate * g_W),\n",
      "           (b, b - learning_rate * g_b)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 3. Compile expressions to functions\n",
      "train = theano.function(inputs=[x, y],\n",
      "                 outputs=cost,\n",
      "                 updates=updates)\n",
      "\n",
      "validate = theano.function(inputs=[x, y],\n",
      "                outputs=error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Training time!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 4. Perform computation\n",
      "train_monitor = []\n",
      "val_monitor = []\n",
      "\n",
      "n_epochs = 10\n",
      "\n",
      "start_time = time.clock()\n",
      "for epoch in range(n_epochs):\n",
      "    loss = train(train_set_x.eval(), train_set_y.eval())\n",
      "    train_monitor.append(validate(train_set_x.eval(), train_set_y.eval()))\n",
      "    val_monitor.append(validate(valid_set_x.eval(), valid_set_y.eval()))\n",
      "    if epoch%2 == 0:\n",
      "        print \"Iteration: \", epoch\n",
      "        print \"Training error, validation error: \", train_monitor[-1], val_monitor[-1]\n",
      "end_time = time.clock()    \n",
      "print 'The code ran for %f seconds' % ((end_time - start_time))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(train_monitor, c='r')\n",
      "plt.plot(val_monitor, c='b')\n",
      "plt.xlabel('Number of Epochs')\n",
      "plt.ylabel('Missclassification')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# optimize run time\n",
      "# dont bother understanding this\n",
      "# do the evaluation once and save it in a normal non-shared variable that we can call our functions with\n",
      "\n",
      "# also could:\n",
      "# As we already know the number of epochs we are going to train for, \n",
      "# we could also fix the size of our monitor variables beforehand and then fill them during training.\n",
      "\n",
      "W.set_value(np.zeros((28*28, 10),\n",
      "                dtype=theano.config.floatX)) \n",
      "b.set_value(np.zeros((10,),\n",
      "                dtype=theano.config.floatX))\n",
      "\n",
      "train_monitor = []\n",
      "val_monitor = []\n",
      "\n",
      "n_epochs = 200\n",
      "\n",
      "trainx = train_set_x.eval()\n",
      "trainy = train_set_y.eval()\n",
      "valx = valid_set_x.eval()\n",
      "valy = valid_set_y.eval()\n",
      "testx = test_set_x.eval()\n",
      "testy = test_set_y.eval()\n",
      "\n",
      "start_time = time.clock()\n",
      "for epoch in range(n_epochs):\n",
      "    loss = train(trainx, trainy)\n",
      "    train_monitor.append(validate(trainx, trainy))\n",
      "    val_monitor.append(validate(valx,valy))\n",
      "    if epoch%2 == 0:\n",
      "        print \"Iteration: \", epoch\n",
      "        print \"Training error, validation error: \", train_monitor[-1], val_monitor[-1]\n",
      "end_time = time.clock()    \n",
      "print 'The code ran for %f seconds' % ((end_time - start_time))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(train_monitor, c='r')\n",
      "plt.plot(val_monitor, c='b')\n",
      "plt.xlabel('Number of Epochs')\n",
      "plt.ylabel('Missclassification')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Batch Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = theano.function(inputs=[],\n",
      "                 outputs=[cost, error],\n",
      "                 updates=updates,\n",
      "                 givens={x: train_set_x, y: train_set_y}\n",
      "                 )\n",
      "\n",
      "validate = theano.function(inputs=[],\n",
      "                outputs=error,\n",
      "                givens={x: valid_set_x, y: valid_set_y}\n",
      "                )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#do 20 and then 100 epochs as one batch iteration:\n",
      "batch_size = 20\n",
      "n_train_batches = train_set_x.shape[0].eval() / batch_size\n",
      "index = T.iscalar() \n",
      "\n",
      "train_batch = theano.function(inputs=[index],\n",
      "                       outputs=cost, \n",
      "                       updates=updates,\n",
      "                       givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 4. Perform computation\n",
      "# reset W and b to zero\n",
      "W.set_value(np.zeros((28*28, 10),\n",
      "                dtype=theano.config.floatX)) \n",
      "b.set_value(np.zeros((10,),\n",
      "                dtype=theano.config.floatX))\n",
      "\n",
      "train_monitor_batch = []\n",
      "val_monitor_batch = []\n",
      "n_epochs = 10\n",
      "\n",
      "start_time = time.clock()\n",
      "for epoch in range(n_epochs):\n",
      "    for batch_index in range(n_train_batches):\n",
      "        loss = train_batch(batch_index)    \n",
      "    val_monitor_batch.append(validate())\n",
      "    train_monitor_batch.append(train())\n",
      "    \n",
      "    if epoch%1 == 0:\n",
      "        print \"Iteration: \", epoch\n",
      "        #print \"Validation error: \",val_monitor_batch[-1]\n",
      "        print \"Training error, validation error: \", train_monitor_batch[-1][1], val_monitor_batch[-1]\n",
      "end_time = time.clock()    \n",
      "print 'The code ran for %f seconds' % ((end_time - start_time))    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now also draws cost:\n",
      "#plt.plot(train_monitor_batch, c='r')\n",
      "#do same with larger batch above..\n",
      "plt.plot(val_monitor_batch, c='b')\n",
      "plt.xlabel('Number of Epochs')\n",
      "plt.ylabel('Missclassification')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "learning rate decay"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What we had is commented with #\n",
      "# learning_rate = 0.1\n",
      "batch_size = 200\n",
      "learning_rate = np.array(0.1, dtype=theano.config.floatX)\n",
      "shared_learning_rate = theano.shared(value=learning_rate, name='lr')\n",
      "\n",
      "#updates = [(W, W - learning_rate * g_W),\n",
      "#           (b, b - learning_rate * g_b)]\n",
      "\n",
      "updates = [(W, W - shared_learning_rate * g_W),\n",
      "           (b, b - shared_learning_rate * g_b),\n",
      "           (shared_learning_rate, shared_learning_rate * 0.995)]\n",
      "\n",
      "#unchanged:\n",
      "train_batch = theano.function(inputs=[index],\n",
      "                       outputs=cost, \n",
      "                       updates=updates,\n",
      "                       givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#gives:\n",
      "for i in xrange(4):\n",
      "    print shared_learning_rate.get_value()\n",
      "    train_batch(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Enough of Logistic regression, on with the layers!!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we'll use the pre-defined class `LogisticRegression` from the [Theano deep learning tutorial](http://deeplearning.net/tutorial/logreg.html) as our last classification layer. \n",
      "\n",
      "To make our model deeper we need to define a hidden layer. \n",
      "\n",
      "definition of the `HiddenLayer` class from the [Theano deep learning tutorial](http://deeplearning.net/tutorial/mlp.html).\n",
      "\n",
      "recap:\n",
      "\n",
      "![MLP](http://deeplearning.net/tutorial/_images/mlp.png)\n",
      "\n",
      "The hidden layer computes the function $s(b+Wx)$, where $s$ is our activation function, $b$ are the biases and $W$ is the weight matrix. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#copied from Thenao DL tut:\n",
      "from logistic_sgd import LogisticRegression\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "\n",
      "        self.input = input\n",
      "\n",
      "        if W is None:\n",
      "            W_values = np.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-np.sqrt(6. / (n_in + n_out)),\n",
      "                    high=np.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "So now we got a Log Regr + a hidden layer Class. Time to put it together"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multi Layer Perceptron:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP(object):\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "\n",
      "        self.hiddenLayer = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Input for the logistic regression layer = the output of the hidden layer. \n",
      "\n",
      "This way the two layers are connected in the model. \n",
      "\n",
      "This means we can just use the error and loss functions from the logistic regression layer as the error and loss function of our whole model. \n",
      "\n",
      "as error of the logistic regression layer is defined in terms of the layer's input. \n",
      "\n",
      "This input now is connected to the output of the hidden layer, thus computing the gradient of the loss function will unravel the whole network all the way down to the first layer's input, which is our image data.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Now we need to define a new training function along with new functions for testing and validation. We use two separate data sets for validation and testing. The reason is that deep network classifiers have a lot of parameters to tune. We use the training data for the actual gradient decent optimization, but we also have to tune the hyper-parameter, like how many layers to choose, how many units in each layer, which learning rate, activation funtion, etc. In a way this is like a whole second level of optimizing our network. This means our validation data can be seen as part of the training data, just for training different parameters than the training data. The test data then is there to test the generalization performance our our trained network. If you are strict about it, you are only allowed to compute the test error once all parameters in the network are fixed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_hidden = 100\n",
      "classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=28 * 28,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=10\n",
      "    )\n",
      "\n",
      "cost = classifier.negative_log_likelihood(y)\n",
      "\n",
      "n_validation_batches= valid_set_x.shape[0].eval() / batch_size\n",
      "\n",
      "validate_model = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.errors(y),\n",
      "    givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "test_model = theano.function(\n",
      "    inputs=[],\n",
      "    outputs=classifier.errors(y),\n",
      "    givens={\n",
      "            x: test_set_x,\n",
      "            y: test_set_y\n",
      "        }\n",
      "    )\n",
      "\n",
      "\n",
      "gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "learning_rate = 0.1\n",
      "updates = [(param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "\n",
      "train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Party time!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time = time.clock()\n",
      "\n",
      "n_epochs = 20\n",
      "start_time = time.clock()\n",
      "\n",
      "for epoch in xrange(n_epochs):\n",
      "    for minibatch_index in xrange(n_train_batches):\n",
      "        minibatch_avg_cost = train_model(minibatch_index)\n",
      "    \n",
      "    validation_losses = [validate_model(i) for i\n",
      "                                     in xrange(n_validation_batches)]\n",
      "    \n",
      "    this_validation_loss = np.mean(validation_losses)\n",
      "    print('epoch %i, minibatches %i, validation error %f %%' %\n",
      "                    (\n",
      "                        epoch,\n",
      "                        n_train_batches,\n",
      "                        this_validation_loss * 100.\n",
      "                    )\n",
      "                )\n",
      "\n",
      "end_time = time.clock()   \n",
      "print 'The code ran for %f seconds' % ((end_time - start_time))    \n",
      "\n",
      "print \"Test error is %f %%\" % (test_model() * 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So whats happening in the hidden layer?\n",
      "\n",
      "let's visualize it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filters = tile_raster_images(classifier.hiddenLayer.W.T.eval(), img_shape=(28,28), tile_shape=(10,10), tile_spacing=(0, 0),\n",
      "                       scale_rows_to_unit_interval=True,\n",
      "                       output_pixel_vals=True)\n",
      "\n",
      "plt.imshow(filters)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "visualize activations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hidden_activation = classifier.hiddenLayer.output\n",
      "compute_activations = theano.function(inputs=[],\n",
      "                                      outputs=hidden_activation,\n",
      "                                      givens={x: valid_set_x})\n",
      "\n",
      "activation_matrix = compute_activations()\n",
      "plt.imshow(activation_matrix[:100,:])\n",
      "plt.xlabel(\"hidden unit\")\n",
      "plt.ylabel(\"samples\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Adding even more Layers\n",
      "According to the universal approximation theorem a single hidden layer neural network with a linear output unit can approximate any continuous function arbitrarily well, given enough hidden units. \n",
      "\n",
      "The result applies for sigmoid, tanh and many other hidden layer activation functions. While this is a great result and sounds like we don't really need to bother about adding more layers, it can be very hard in practice to find the right local minimum of such a network. It also doesn't specify how many hidden units are enough. \n",
      "\n",
      "It is computationally more feasible to stack moderate size hidden layers on top of each other, than to try and optimize a gigantic single hidden layer. So finally here is the generalization of our `MLP` class for an arbitrary number of hidden layers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP_deep(object):\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \n",
      "        self.hiddenLayers = []\n",
      "        \n",
      "        self.params = []\n",
      "        \n",
      "        next_input = input\n",
      "        next_n_in = n_in\n",
      "        \n",
      "        for n_h in n_hidden:\n",
      "            hl = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=next_input,\n",
      "            n_in=next_n_in,\n",
      "            n_out=n_h,\n",
      "            activation=T.tanh\n",
      "            )\n",
      "            next_input = hl.output\n",
      "            next_n_in = n_h\n",
      "            self.hiddenLayers.append(hl)\n",
      "            self.params += hl.params                         \n",
      "\n",
      "\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayers[-1].output,\n",
      "            n_in=n_hidden[-1],\n",
      "            n_out=n_out\n",
      "        )\n",
      "\n",
      "        self.negative_log_likelihood = self.logRegressionLayer.negative_log_likelihood\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        self.params += self.logRegressionLayer.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "again lets connect the layers by by specifying the output of the lower layer as input for the new layer on top:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#2 hidden layers, one with 200n and other with 100\n",
      "n_hidden = [200,100]\n",
      "\n",
      "classifier = MLP_deep(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=28 * 28,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=10\n",
      "    )\n",
      "\n",
      "cost = classifier.negative_log_likelihood(y)\n",
      "\n",
      "n_validation_batches= valid_set_x.shape[0].eval() / batch_size\n",
      "\n",
      "validate_model = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.errors(y),\n",
      "    givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "test_model = theano.function(\n",
      "    inputs=[],\n",
      "    outputs=classifier.errors(y),\n",
      "    givens={\n",
      "            x: test_set_x,\n",
      "            y: test_set_y\n",
      "        }\n",
      "    )\n",
      "\n",
      "\n",
      "gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "learning_rate = 0.1\n",
      "updates = [(param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "\n",
      "train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "liftoff!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time = time.clock()\n",
      "\n",
      "n_epochs = 20\n",
      "start_time = time.clock()\n",
      "\n",
      "val_monitor_batch = []\n",
      "\n",
      "for epoch in xrange(n_epochs):\n",
      "    for minibatch_index in xrange(n_train_batches):\n",
      "        minibatch_avg_cost = train_model(minibatch_index)\n",
      "    \n",
      "    validation_losses = [validate_model(i) for i\n",
      "                                     in xrange(n_validation_batches)]\n",
      "    this_validation_loss = np.mean(validation_losses)\n",
      "    val_monitor_batch.append(this_validation_loss)\n",
      "    print('epoch %i, minibatches %i, validation error %f %%' %\n",
      "                    (\n",
      "                        epoch,\n",
      "                        n_train_batches,\n",
      "                        this_validation_loss * 100.\n",
      "                    )\n",
      "                )\n",
      "\n",
      "end_time = time.clock()   \n",
      "print 'The code ran for %f seconds' % ((end_time - start_time))    \n",
      "\n",
      "print \"Test error is %f %%\" % (test_model() * 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filters = tile_raster_images(classifier.hiddenLayers[0].W.T.eval(), img_shape=(28,28), tile_shape=(10,10), tile_spacing=(0, 0),\n",
      "                       scale_rows_to_unit_interval=True,\n",
      "                       output_pixel_vals=True)\n",
      "\n",
      "plt.imshow(filters)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#if we got time left lets play with the DNN...."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now also draws cost:\n",
      "#plt.plot(train_monitor_batch, c='r')\n",
      "#do same with larger batch above..\n",
      "plt.plot(val_monitor_batch, c='b')\n",
      "plt.xlabel('Number of Epochs')\n",
      "plt.ylabel('Missclassification')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}