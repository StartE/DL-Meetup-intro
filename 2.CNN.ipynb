{
 "metadata": {
  "name": "",
  "signature": "sha256:aba1bb65a10c75807bfbd9c4a2f572039f6b5fecfc45f9de525e15dd6f6851f0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "matplotlib.pyplot.gray()\n",
      "\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano import shared, function\n",
      "theano.config.floatX = 'float32'\n",
      "rng = np.random.RandomState(42)\n",
      "\n",
      "import sys\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will make heavy use of the resources in the Theano deep learning tutorial. We have it integrated into our git repository as a submodule. You can clone the git repository by doing the following steps:\n",
      "(be sure to include the `--recursive` or you won't get the Theano deep learning tutorial)\n",
      "\n",
      "`git clone --recursive https://github.com/graphific/DL-Meetup-intro.git`\n",
      "\n",
      "If you already cloned the repository, but the `DeepLearningTutorial` folder is empty, you need to checkout the submodule. Make sure you are in the folder `ComputeFest2015_DeepLearning` and then execute the following command:\n",
      "\n",
      "`git submodule update --init --recursive`\n",
      "\n",
      "\n",
      "Now we have to add this directory to the PythonPath. Depending on the location of your git repository you might have to change this path."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sys.path.insert(1,'DeepLearningTutorials/code')\n",
      "#sys.path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MNIST: Handwritten Digit Recognition \n",
      "MNIST consists of 70 000 small image patches, each showing a handwritten digit character in white on a black background. There are 10 different classes (the digits from 0-9). \n",
      "\n",
      "Let's load the data and have a look."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from logistic_sgd import load_data\n",
      "dataset='mnist.pkl.gz'\n",
      "\n",
      "## If not already cached this function actually downloads the data\n",
      "datasets = load_data(dataset)\n",
      "\n",
      "train_set_x, train_set_y = datasets[0]\n",
      "valid_set_x, valid_set_y = datasets[1]\n",
      "test_set_x, test_set_y = datasets[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Size of the training data matrix: \", train_set_x.get_value().shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data is stored with one training sample per row. The original image patches are $28 \\times 28$ pixels, hence we have $28 \\cdot 28 = 784$ feature columns in our data matrix. We can reshape each row of our data matrix back to the original $28 \\times 28$ image patch to visualize what the data looks like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import tile_raster_images\n",
      "\n",
      "samples = tile_raster_images(train_set_x.get_value(), img_shape=(28,28), tile_shape=(5,10), tile_spacing=(0, 0),\n",
      "                       scale_rows_to_unit_interval=True,\n",
      "                       output_pixel_vals=True)\n",
      "\n",
      "plt.imshow(samples)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Convolutional Network\n",
      "\n",
      "We'll create a convolutional network to classify digits.  We use code from the Deep Network Tutorials to make this a little easier, particularly the Convolutional Layer class **`LeNetConvPoolLayer`**, [defined in convolutional_mlp.py](https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/convolutional_mlp.py#L39).\n",
      "\n",
      "We'll build up a network using the following layers:\n",
      "* input (1 channel)\n",
      "* 10  1x9x9 filters   &mdash; (first dimension is 1 because the input image is grayscale)\n",
      "* 10  10x5x5 filters  &mdash; (first dimenions is 10 because the previous layer had 10 filters)\n",
      "* a fully connected sigmoidal layer with 20 outputs\n",
      "* a fully connected Logistic Regression layer with 10 outputs\n",
      "\n",
      "We'll train with batches, with a batch size of 10. (feel free to change it to a higher number, but for demo purposes 100 will be ok enough and faster)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Setup for the Convolutional Network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup 1: parameters of the network and training\n",
      "from convolutional_mlp import LeNetConvPoolLayer\n",
      "from mlp import HiddenLayer\n",
      "from logistic_sgd import LogisticRegression\n",
      "\n",
      "# network parameters\n",
      "num_kernels = [10, 10]\n",
      "kernel_sizes = [(9, 9), (5, 5)]\n",
      "sigmoidal_output_size = 20\n",
      "\n",
      "# training parameters\n",
      "learning_rate = 0.1\n",
      "batch_size = 100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For efficiency, we break the data into batches.  Our network will take **`batch_size`** images at a time as input."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup 2: compute batch sizes for train/test/validation\n",
      "\n",
      "# borrow=True gets us the value of the variable without making a copy.\n",
      "n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
      "n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
      "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
      "n_train_batches /= batch_size\n",
      "n_test_batches /= batch_size\n",
      "n_valid_batches /= batch_size "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "fast hack to limit datasize, feel free to skip this step:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#fast hack to limit datasize, feel free to skip this step:\n",
      "n_train_batches /= 10\n",
      "n_test_batches /= 10\n",
      "n_valid_batches /= 10\n",
      "n_train_batches /= 10\n",
      "n_test_batches /= 10\n",
      "n_valid_batches /= 10 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "/end hack"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup 3.\n",
      "\n",
      "# Declare inputs to network - x and y are placeholders\n",
      "# that will be used in the training/testing/validation functions below.\n",
      "x = T.matrix('x')  # input image data\n",
      "y = T.ivector('y') # input label data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Layer 0 - First convolutional Layer\n",
      "\n",
      "The first layer takes **`(batch_size, 1, 28, 28)`** as input, convolves it with **10** different **9x9** filters, and then downsamples (via maxpooling) in a **2x2** region.  Each filter/maxpool combination produces an output of size **`(28-9+1)/2 = 10`** on a side.\n",
      "\n",
      "The size of the first layer's output is therefore **`(batch_size, 10, 10, 10)`**. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layer0_input_size = (batch_size, 1, 28, 28)  # fixed size from the data\n",
      "edge0 = (28 - kernel_sizes[0][0] + 1) / 2\n",
      "layer0_output_size = (batch_size, num_kernels[0], edge0, edge0)\n",
      "# check that we have an even multiple of 2 before pooling\n",
      "assert ((28 - kernel_sizes[0][0] + 1) % 2) == 0\n",
      "\n",
      "# The actual input is the placeholder x reshaped to the input size of the network\n",
      "layer0_input = x.reshape(layer0_input_size)\n",
      "\n",
      "layer0 = LeNetConvPoolLayer(rng,\n",
      "                            input=layer0_input,\n",
      "                            image_shape=layer0_input_size,\n",
      "                            filter_shape=(num_kernels[0], 1) + kernel_sizes[0],\n",
      "                            poolsize=(2, 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Layer 1 - Second convolutional Layer\n",
      "\n",
      "The second layer takes **`(batch_size, 10, 10, 10)`** as input, convolves it with 10 different **10x5x5** filters, and then downsamples (via maxpooling) in a **2x2** region.  Each filter/maxpool combination produces an output of size **`(10-5+1)/2 = 3`** on a side.\n",
      "\n",
      "The size of the second layer's output is therefore **`(batch_size, 10, 3, 3)`**. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layer1_input_size = layer0_output_size\n",
      "edge1 = (edge0 - kernel_sizes[1][0] + 1) / 2\n",
      "layer1_output_size = (batch_size, num_kernels[1], edge1, edge1)\n",
      "# check that we have an even multiple of 2 before pooling\n",
      "assert ((edge0 - kernel_sizes[1][0] + 1) % 2) == 0\n",
      "\n",
      "layer1 = LeNetConvPoolLayer(rng,\n",
      "                            input=layer0.output,\n",
      "                            image_shape=layer1_input_size,\n",
      "                            filter_shape=(num_kernels[1], num_kernels[0]) + kernel_sizes[1],\n",
      "                            poolsize=(2, 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Layer 2 - Fully connected sigmoidal layer\n",
      "\n",
      "The sigmoidal layer takes a vector as input.\n",
      "\n",
      "We flatten all but the first two dimensions, to get an input of size **`(batch_size, 30 * 4 * 4)`**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layer2_input = layer1.output.flatten(2)\n",
      "\n",
      "layer2 = HiddenLayer(rng,\n",
      "                     input=layer2_input,\n",
      "                     n_in=num_kernels[1] * edge1 * edge1,\n",
      "                     n_out=sigmoidal_output_size,\n",
      "                     activation=T.tanh)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Layer 3 - Logistic regression output layer\n",
      "\n",
      "A fully connected logistic regression layer converts the sigmoid's layer output to a class label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layer3 = LogisticRegression(input=layer2.output,\n",
      "                            n_in=sigmoidal_output_size,\n",
      "                            n_out=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Training the network\n",
      "\n",
      "To train the network, we have to define a cost function.  We'll use the Negative Log Likelihood of the model, relative to the true labels **`y`**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The cost we minimize during training is the NLL of the model.\n",
      "# Recall: y is a placeholder we defined above\n",
      "cost = layer3.negative_log_likelihood(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Gradient descent\n",
      "\n",
      "We will train with Stochastic Gradient Descent.  To do so, we need the gradient of the cost relative to the parameters of the model.  We can get the parameters for each label via the **`.params`** attribute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a list of all model parameters to be fit by gradient descent\n",
      "params = layer3.params + layer2.params + layer1.params + layer0.params\n",
      "\n",
      "# create a list of gradients for all model parameters\n",
      "grads = T.grad(cost, params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training function\n",
      "\n",
      "The actual training function takes a batch of inputs (images **`x`** and labels **`y`**), and updates the model parameters with a small step in the steepest direction that reduces the gradient.\n",
      "\n",
      "This is the\n",
      "**`(param_i, param_i - learning_rate * grad_i)`**\n",
      "line in the code below.\n",
      "\n",
      "We use the **`updates`** keyword to **`theano.function()`** to update these variables in-place.  See [the documentation for function()](http://deeplearning.net/software/theano/library/compile/function.html).\n",
      "\n",
      "We also make use of the **`givens`** keyword, to alias **`x`** and **`y`** to batches (aka, slices) of  **`train_set_x`** and **`train_set_y`**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train_model is a function that updates the model parameters by SGD, and returns the current cost\n",
      "#\n",
      "# We create the updates list by automatically looping over all\n",
      "# (params[i], grads[i]) pairs.\n",
      "updates = [\n",
      "    (param_i, param_i - learning_rate * grad_i)  # <=== SGD update step\n",
      "    for param_i, grad_i in zip(params, grads)\n",
      "]\n",
      "\n",
      "index = T.lscalar()  # index to a batch of training/validation/testing examples\n",
      "\n",
      "train_model = theano.function(\n",
      "    [index],\n",
      "    cost,\n",
      "    updates=updates,\n",
      "    givens={\n",
      "        x: train_set_x[index * batch_size: (index + 1) * batch_size],  # <=== batching\n",
      "        y: train_set_y[index * batch_size: (index + 1) * batch_size]   # <=== batching\n",
      "    }\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Validation function\n",
      "\n",
      "To track progress on a held-out set, we count the number of misclassified examples in the validation set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "validate_model = theano.function(\n",
      "        [index],\n",
      "        layer3.errors(y),\n",
      "        givens={\n",
      "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Test function\n",
      "\n",
      "After training, we check the number of misclassified examples in the test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_model = theano.function(\n",
      "    [index],\n",
      "    layer3.errors(y),\n",
      "    givens={\n",
      "        x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "        y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Training loop\n",
      "\n",
      "We use SGD for a fixed number of iterations over the full training set (an \"epoch\").  Usually, we'd use a more complicated rule, such as iterating until a certain number of epochs fail to produce improvement in the validation set.  \n",
      "Were doing CPU so will take a while here, lets do only 2 epochs, feel free to try more yourself, while I'm talking :)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#if you've limited the datasize above then you can easily do 200 epochs, \n",
      "#but in any case accuracy will be bad as NN only works with enough data:\n",
      "#keeping the dataset intact with batch_size of 100, and 20 training epochs,\n",
      "# validation error around x and test should be at least near x\n",
      "for epoch in range(10):\n",
      "    costs = [train_model(i) for i in xrange(n_train_batches)]\n",
      "    validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
      "    print \"Epoch {}    NLL {:.2}    %err in validation set {:.1%}\".format(epoch + 1, np.mean(costs), np.mean(validation_losses))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Learned features\n",
      "\n",
      "We can visualize the filters learned by the first layer.  In most vision tasks, these will be simple features, such as lines, points, or edges (such as in Figure 2 of [this paper](http://www.cs.toronto.edu/~rgrosse/icml09-cdbn.pdf)).\n",
      "\n",
      "The MNIST data doesn't contain as much variation as natural images, so we don't see as obvious edge features, but we still see some stroke-like features appearing in the lowest level.  With some standard tricks (adding noise to the training set, shifting examples around, using Dropout), we'd probably see smoother features (as in the paper linked above)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#first layer, layer 0\n",
      "filters = tile_raster_images(layer0.W.get_value(borrow=True), img_shape=(9, 9), tile_shape=(2,5), tile_spacing=(3, 3),\n",
      "                       scale_rows_to_unit_interval=True,\n",
      "                       output_pixel_vals=True)\n",
      "\n",
      "plt.imshow(filters)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# layer 1\n",
      "filters = tile_raster_images(layer1.W.get_value(borrow=True), img_shape=(50, 5), tile_shape=(1,10), tile_spacing=(3, 3),\n",
      "                       scale_rows_to_unit_interval=False,\n",
      "                       output_pixel_vals=False)\n",
      "\n",
      "plt.imshow(filters)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sigmoid layer\n",
      "filters = tile_raster_images(layer2.W.get_value(borrow=True), img_shape=(1, 20), tile_shape=(10,10), tile_spacing=(3, 3),\n",
      "                       scale_rows_to_unit_interval=False,\n",
      "                       output_pixel_vals=False)\n",
      "\n",
      "plt.imshow(filters)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#final layer: fully connected logistic regression layer converts the sigmoid's layer output to a class label.\n",
      "filters = tile_raster_images(layer3.W.get_value(borrow=True), img_shape=(1, 10), tile_shape=(4,5), tile_spacing=(3, 3),\n",
      "                       scale_rows_to_unit_interval=False,\n",
      "                       output_pixel_vals=False)\n",
      "\n",
      "plt.imshow(filters)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Check performance on the test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_errors = [test_model(i) for i in range(n_test_batches)]\n",
      "print \"test errors: {:.1%}\".format(np.mean(test_errors))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}